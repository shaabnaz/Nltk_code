{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocessong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"\"this is paragraph about childhood natural and habbit,which leads a person forcoming life,.now i m\n",
    "telling some thing very a very dangerous about natur\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"this is paragraph about childhood natural and habbit,which leads a person forcoming life,.now i m\\ntelling some thing very a very dangerous about natur']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"this is paragraph about childhood natural and habbit,which leads a person forcoming life,.now i m\n",
      "telling some thing very a very dangerous about natur\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'this', 'is', 'paragraph', 'about', 'childhood', 'natural', 'and', 'habbit', ',', 'which', 'leads', 'a', 'person', 'forcoming', 'life', ',', '.now', 'i', 'm', 'telling', 'some', 'thing', 'very', 'a', 'very', 'dangerous', 'about', 'natur']\n"
     ]
    }
   ],
   "source": [
    "documents=word_tokenize(corpus)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``\n",
      "this\n",
      "is\n",
      "paragraph\n",
      "about\n",
      "childhood\n",
      "natural\n",
      "and\n",
      "habbit\n",
      ",\n",
      "which\n",
      "leads\n",
      "a\n",
      "person\n",
      "forcoming\n",
      "life\n",
      ",\n",
      ".now\n",
      "i\n",
      "m\n",
      "telling\n",
      "some\n",
      "thing\n",
      "very\n",
      "a\n",
      "very\n",
      "dangerous\n",
      "about\n",
      "natur\n"
     ]
    }
   ],
   "source": [
    "for i in  documents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'this',\n",
       " 'is',\n",
       " 'paragraph',\n",
       " 'about',\n",
       " 'childhood',\n",
       " 'natural',\n",
       " 'and',\n",
       " 'habbit',\n",
       " ',',\n",
       " 'which',\n",
       " 'leads',\n",
       " 'a',\n",
       " 'person',\n",
       " 'forcoming',\n",
       " 'life',\n",
       " ',.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'm',\n",
       " 'telling',\n",
       " 'some',\n",
       " 'thing',\n",
       " 'very',\n",
       " 'a',\n",
       " 'very',\n",
       " 'dangerous',\n",
       " 'about',\n",
       " 'natur']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer #particularly those suited for linguistic parsing.\n",
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'this',\n",
       " 'is',\n",
       " 'paragraph',\n",
       " 'about',\n",
       " 'childhood',\n",
       " 'natural',\n",
       " 'and',\n",
       " 'habbit',\n",
       " ',',\n",
       " 'which',\n",
       " 'leads',\n",
       " 'a',\n",
       " 'person',\n",
       " 'forcoming',\n",
       " 'life',\n",
       " ',',\n",
       " '.now',\n",
       " 'i',\n",
       " 'm',\n",
       " 'telling',\n",
       " 'some',\n",
       " 'thing',\n",
       " 'very',\n",
       " 'a',\n",
       " 'very',\n",
       " 'dangerous',\n",
       " 'about',\n",
       " 'natur']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why Stemming is Useful\n",
    "- Simplifies Analysis: Reduces inflectional and derivational forms of a word to a common base, making text easier to analyze.\n",
    "- Speeds up Processes: Helps in text normalization for tasks like information retrieval, text classification, and sentiment analysis.\n",
    "- Reduces Vocabulary Size: Treats related words as the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "# to reduce a word to its base or root form, known as the stem. The idea is to remove prefixes and suffixes (e.g., ing, ly, es, s, etc.)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"running\"))  # Output: run\n",
    "print(stemmer.stem(\"better\"))   # Output: better\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "bet\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer #More aggressive than the Porter Stemmer.\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem(\"running\"))  # Output: run\n",
    "print(stemmer.stem(\"better\"))   # Output: bet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'run', 'easili', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"runner\", \"runs\", \"easily\", \"fairness\"]\n",
    "\n",
    "# Apply stemming\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eat', 'love', 'go', 'run']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stem=RegexpStemmer('ing$|s$|es$|able$',min=4) #Ensures that stemming will only apply to words with at least 4 characters.\n",
    "words=[\"eating\",\"loveable\",\"goes\",\"runs\"]\n",
    "stems = [reg_stem.stem(word) for word in words]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The Snowball Stemmer is a stemming algorithm used for reducing words to their root forms while maintaining a balance between precision and simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'jump', 'easili', 'fair', 'studi', 'caress']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Create a Snowball Stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# List of words to stem\n",
    "words = [\"running\", \"jumps\", \"easily\", \"fairness\", \"studied\", \"caresses\"]\n",
    "\n",
    "# Apply stemming\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- . Unlike stemming, which simply trims affixes, lemmatization uses a dictionary-like approach to ensure the resulting base word is meaningful and valid in context. for example stemming give better to bett but lemmatization give better to good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize words with different POS tags\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # Verb: \"run\" pos me an part of speech\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Adjective: \"good\"\n",
    "print(lemmatizer.lemmatize(\"cats\", pos=\"n\"))    # Noun: \"cat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"A rainy day brings with it a unique charm,\n",
    ", painting the world in hues of grey and green.\n",
    "The rhythmic sound of raindrops tapping on windows and rooftops creates a soothing symphony that. often inspires reflection and creativity. \n",
    "Streets glisten as puddles form, and the earthy aroma of wet soil fills the air, \n",
    "evoking a sense of nostalgia. For some,\n",
    " it is a day to curl up with a warm cup of tea and a good book,\n",
    "   while others embrace the rain, walking beneath colorful umbrellas or simply enjoying the cool, refreshing downpour. Despite the occasional inconvenience, a rainy day has its own beauty,\n",
    " offering a gentle pause from the usual bustle of life.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemer=PorterStemmer()\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words= [stemmer.stem(word) for word in words if word  not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a raini day bring uniqu charm , , paint world hue grey green .',\n",
       " 'the rhythmic sound raindrop tap window rooftop creat sooth symphoni .',\n",
       " 'often inspir reflect creativ .',\n",
       " 'street glisten puddl form , earthi aroma wet soil fill air , evok sens nostalgia .',\n",
       " 'for , day curl warm cup tea good book , other embrac rain , walk beneath color umbrella simpli enjoy cool , refresh downpour .',\n",
       " 'despit occasion inconveni , raini day beauti , offer gentl paus usual bustl life .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"A rainy day brings with it a unique charm,\n",
    ", painting the world in hues of grey and green.\n",
    "The rhythmic sound of raindrops tapping on windows and rooftops creates a soothing symphony that. OTHEN inspires reflection and creativity. \n",
    "Streets glisten as puddles form, and the earthy aroma of wet soil fills the air, \n",
    "evoking a sense of nostalgia. For some,\n",
    " it is a day to curl up with a warm cup of tea and a good book,\n",
    "   while others embrace the rain, walking beneath colorful umbrellas or simply enjoying the cool, refreshing downpour. Despite the occasional inconvenience, a rainy day has its own beauty,\n",
    " offering a gentle pause from the usual bustle of life. This is example of better Gestur\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Create a Snowball Stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word  not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a raini day bring uniqu charm , , paint world hue grey green .',\n",
       " 'the rhythmic sound raindrop tap window rooftop creat sooth symphoni .',\n",
       " 'othen inspir reflect creativ .',\n",
       " 'street glisten puddl form , earthi aroma wet soil fill air , evok sens nostalgia .',\n",
       " 'for , day curl warm cup tea good book , other embrac rain , walk beneath color umbrella simpli enjoy cool , refresh downpour .',\n",
       " 'despit occasion inconveni , raini day beauti , offer gentl paus usual bustl life .',\n",
       " 'this exampl better gestur']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemitizer=WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemitizer.lemmatize(word.lower(),pos='v') for word in words if word  not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a rainy day bring unique charm , , paint world hue grey green .',\n",
       " 'the rhythmic sound raindrops tap windows rooftops create soothe symphony .',\n",
       " 'othen inspire reflection creativity .',\n",
       " 'streets glisten puddle form , earthy aroma wet soil fill air , evoke sense nostalgia .',\n",
       " 'for , day curl warm cup tea good book , others embrace rain , walk beneath colorful umbrellas simply enjoy cool , refresh downpour .',\n",
       " 'despite occasional inconvenience , rainy day beauty , offer gentle pause usual bustle life .',\n",
       " 'this example better gestur']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pos: part of speech\n",
    "- they are near about 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('baby', 'NN'), ('is', 'VBZ'), ('weeping', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The baby is weeping\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "- How to Perform NER in NLTK\n",
    "- NLTK uses a tree-based chunking approach for NER. It requires:\n",
    "\n",
    "- Tokenization\n",
    "- Part-of-speech (POS) tagging\n",
    "- Chunking to identify entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Mr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Mr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')  # Additional resource required for NER\n",
    "print(\"Resources downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Barack Obama was the 44th President of the United States and was born in Hawaii.679$ and 22_12_22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  44th/JJ\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  and/CC\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  Hawaii.679/NNP\n",
      "  $/$\n",
      "  and/CC\n",
      "  22_12_22/CD)\n"
     ]
    }
   ],
   "source": [
    "named_entities = ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .draw() Method in NLTK\n",
    "- The .draw() method in NLTK is used to visualize a parse tree or a chunk tree in a GUI window. It is particularly useful for - -       understanding the hierarchical structure of chunked elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Mark Zuckerberg founded Facebook in the United States.\"\n",
    "\n",
    "# Tokenize and Part-of-Speech Tagging\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "chunk_tree = ne_chunk(pos_tags)\n",
    "\n",
    "# Visualize the chunk tree\n",
    "chunk_tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse matrix is matrix which contain majority of zeros and leads to over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In NLTK (Natural Language Toolkit), text can be converted into vectors using several methods\n",
    "- . Bag of Words (BoW) Representation\n",
    "-  TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- Word Embeddings (Word2Vec)\n",
    "- One-Hot Encoding\n",
    "- Use Bag of Words or TF-IDF for simple text classification tasks.\n",
    "- Use Word2Vec or other embeddings for more advanced tasks like semantic similarity or deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['ball' 'cat' 'document' 'hat' 'is' 'my' 'this']\n",
      "BoW vectors:\n",
      " [[0 0 0 0 4 0 5]\n",
      " [0 0 1 0 0 0 1]\n",
      " [1 2 0 1 3 4 4]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example texts\n",
    "documents = [\"This is a  this is  this is this this is.\", \"This document.\",\"this my cat this is my ball this is my hat this is my cat\"]\n",
    "\n",
    "# Tokenize using NLTK (optional, CountVectorizer handles tokenization too)\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Use CountVectorizer to create the Bag of Words representation\n",
    "vectorizer = CountVectorizer()\n",
    "bow_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "# BoW vectors\n",
    "print(\"BoW vectors:\\n\", bow_vectors.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are some applications that use TF-IDF:\n",
    "\n",
    "- Search engines (e.g., Google, Bing).\n",
    "- Spam detection systems.\n",
    "- Text classification tools.\n",
    "- Document clustering platforms.\n",
    "- Keyword extraction tools.\n",
    "- Text summarization tools.\n",
    "- Recommender systems (e.g., for articles or products).\n",
    "- Plagiarism detection tools (e.g., Turnitin).\n",
    "- Semantic search engines (e.g., Elasticsearch).\n",
    "- Chatbots with knowledge retrieval capabilities.\n",
    "- Social media analysis tools (e.g., for trends or opinion mining).\n",
    "- Fraud detection systems for textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i' 'in' 'love' 'my' 'passion' 'programming' 'python']\n",
      "          i        in      love        my   passion  programming    python\n",
      "0  0.433067  0.336315  0.433067  0.000000  0.000000     0.569431  0.433067\n",
      "1  0.619805  0.481334  0.619805  0.000000  0.000000     0.000000  0.000000\n",
      "2  0.000000  0.345205  0.000000  0.584483  0.584483     0.000000  0.444514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mr\\miniconda3\\envs\\streamlit\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "documents = [\n",
    "    \"I love programming in Python\",\"i love in \",\n",
    "    \"my passion in python\"]\n",
    "\n",
    "# Initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenize,lowercase=True)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (unique terms in the corpus)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_names \n",
    "print(feature_names)\n",
    "\n",
    "# Display the TF-IDF matrix as a DataFrame for better readability\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: [0, 1, 0, 0]\n",
      "love: [0, 0, 1, 0]\n",
      "Python: [0, 0, 0, 1]\n",
      "programming: [1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"I love Python programming\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Create a vocabulary\n",
    "vocabulary = list(set(tokens))\n",
    "\n",
    "# Function to generate one-hot encoding\n",
    "def one_hot_encode(word, vocab):\n",
    "    vector = [0] * len(vocab)  # Initialize a zero vector\n",
    "    index = vocab.index(word)  # Find the index of the word\n",
    "    vector[index] = 1          # Set the corresponding position to 1\n",
    "    return vector\n",
    "\n",
    "# Generate one-hot vectors for each word\n",
    "one_hot_vectors = {word: one_hot_encode(word, vocabulary) for word in tokens}\n",
    "\n",
    "# Display the one-hot encoded vectors\n",
    "for word, vector in one_hot_vectors.items():\n",
    "    print(f\"{word}: {vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding in NLP is a representation of words in a continuous vector space where words with similar meanings are mapped closer to each other. It captures the semantic relationships between words, which one-hot encoding or other basic methods cannot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gensim is an open-source Python library designed for unsupervised learning and natural language processing (NLP). It specializes in - handling large corpora of text data and extracting meaningful relationships between words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"parents and teachers palyed role in life\",\n",
    "    \"they protect child from\",\" i love my mother and my sister\",\"how sibling take care\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "print(tokenized_sentences)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=5, window=5, min_count=1, workers=4)#vector_size=5 mean value of word,\n",
    "#worker mean thread and window size mean\n",
    "\n",
    "# Get the vector for a word=wv word vector\n",
    "vector = model.wv[\"in\"]\n",
    "print(f\"Vector for 'teachers':\\n{vector}\")\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"love\", topn=5) # topn mean three word we can change it\n",
    "print(\"\\nMost similar words to 'love':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Word2Vec Works\n",
    "## Word2Vec comes in two main variants:\n",
    "\n",
    "- CBOW (Continuous Bag of Words): Predicts a target word based on its context words.\n",
    "- Skip-gram: Predicts context words given a target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average word to vector  used for\n",
    "- Text Classification:\n",
    " Sentiment analysis, spam detection.\n",
    "- Text Clustering:\n",
    " Grouping similar documents or sentences.\n",
    "- Information Retrieval:\n",
    " Semantic similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\mr\\miniconda3\\envs\\streamlit\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\mr\\miniconda3\\envs\\streamlit\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\mr\\miniconda3\\envs\\streamlit\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mr\\miniconda3\\envs\\streamlit\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\mr\\miniconda3\\envs\\streamlit\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, KeyedVectors\n",
    "import  gensim.downloader as api\n",
    "wv=api.load('word2vec-google-news-300')\n",
    "\n",
    "\n",
    "# Model is now ready to use!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
